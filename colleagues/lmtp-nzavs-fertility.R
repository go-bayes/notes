










# preliminaries -----------------------------------------------------------


# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
source("/Users/joseph/GIT/templates/functions/libs2.R")

# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
source("/Users/joseph/GIT/templates/functions/funs.R")

# experimental functions
source("/Users/joseph/GIT/templates/functions/experimental_funs.R")


# # ALERT: UNCOMMENT THIS AND DOWNLOAD THE LIBRARIES FROM JB's GITHUB
# source("https://raw.githubusercontent.com/go-bayes/templates/main/functions/libs2.R")
#
# # ALERT: UNCOMMENT THIS AND DOWNLOAD THE FUNCTIONS FROM JB's GITHUB
# source("https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R")
#
#
# # ALERT: UNCOMMENT THIS AND DOWNLOAD THE FUNCTIONS FROM JB's GITHUB
# source(
#   "https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R"
# )



## WARNING SET THIS PATH TO YOUR DATA ON YOUR SECURE MACHINE. DO NOT USE THIS PATH
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs_refactor/nzavs_data_23"
  )

# read data: note that you need use the arrow package in R
dat <- arrow::read_parquet(pull_path)


### WARNING: THIS PATH WILL NOT WORK FOR YOU. PLEASE SET A PATH TO YOUR OWN COMPUTER!! ###
### WARNING: FOR EACH NEW STUDY SET UP A DIFFERENT PATH OTHERWISE YOU WILL WRITE OVER YOUR MODELS

# extreme models
push_mods <-
  fs::path_expand(
    "/Users/joseph/Library/CloudStorage/Dropbox-v-project/data/nzvs_mods/notes/23-radim-nzavs-fertility"
  )


# check path:is this correct?  check so you know you are not overwriting other directors
push_mods

# set exposure here
nzavs_exposure <- "religion_church"


# define exposures --------------------------------------------------------
# define exposure
A <- "t1_religion_church"


# set exposure variable, can be both the continuous and the coarsened, if needed
exposure_var = c("religion_church",
                 "religion_church_round",
                 "religion_church_four",
                 "not_lost") #


# define shift functions --------------------------------------------------



# shift one pont up if under 6
# f_1 <- function (data, trt) data[[trt]] + 1


# Decrease by one point
# f <- function(data, trt) {
#   ifelse(data[[trt]] >= min_score + one_point_in_sd_units, data[[trt]] - one_point_in_sd_units,  min_score)
# }

# set all to minimum
f <- function(data, trt) {
  ifelse(data[[trt]] > 0, 0,  data[[trt]])
}


#  increase everyone by one point, contrasted with what they would be anyway.
# only use this function for raw scores

# set all to at least 4
f_1  <- function(data, trt) {
  ifelse(data[[trt]] < 4, 4,  data[[trt]])
}


# see second function below

# set number of folds for ML here. use a minimum of 5 and a max of 10
SL_folds = 5

#this will allow you to track progress
progressr::handlers(global = TRUE)

# set seed for reproducing results
seed <- 0112358
set.seed(seed)

# set cores for estimation
library(future)
plan(multisession)
n_cores <- parallel::detectCores()

# super learner libraries
sl_lib <- c("SL.glmnet",
            "SL.ranger",
            "SL.xgboost")

#Improve speed (if needed)
# sl_lib_args <- list(
#   SL.glmnet = list(nalpha = 5, nlambda = 20),
#   SL.ranger = list(num.threads = 4),
#   SL.xgboost = list(nthread = 4,
#                     nrounds = 50,
#                     early_stopping_rounds = 10,
#                     max_depth = 6,
#                     colsample_bytree = 0.8,
#                     subsample = 0.8,
#                     eta = 0.01,
#                     tree_method = 'hist')
# )

# superlearner libraries
library(SuperLearner)
library(ranger)
library(xgboost)
library(glmnet)

# boost speed
SL.xgboost = list(tree_method = 'gpu_hist')


# check options
listWrappers()


# kessler 6 ---------------------------------------------------------------
# uncomment to get analysis
#
#
#
# dt_only_k6 <- dt_19 |> select(kessler_depressed, kessler_effort,kessler_hopeless,
#                                  kessler_worthless, kessler_nervous,
#                                  kessler_restless)
#
#
# # check factor structure
# performance::check_factorstructure(dt_only_k6)
#
# # explore a factor structure made of 3 latent variables
# efa <- psych::fa(dt_only_k6, nfactors = 2) %>%
#   model_parameters(sort = TRUE, threshold = "max")
#
# efa
#
#
# n <- n_factors(dt_only_k6)
#
# # plot
# plot(n) + theme_classic()
#
# # CFA
# part_data <- datawizard::data_partition(dt_only_k6, traing_proportion = .7, seed = seed)
#
#
# # set up training data
# training <- part_data$p_0.7
# test <- part_data$test
#
#
# # one factor model
# structure_k6_one <- psych::fa(training, nfactors = 1) |>
#   efa_to_cfa()
#
# # two factor model model
# structure_k6_two <- psych::fa(training, nfactors = 2) |>
#   efa_to_cfa()
#
# # three factor model
# structure_k6_three <- psych::fa(training, nfactors = 3) %>%
#   efa_to_cfa()
#
# # inspect models
# structure_k6_one
# structure_k6_two
# structure_k6_three
#
#
# # Next we perform the confirmatory factor analysis.
#
#
# one_latent <-
#   suppressWarnings(lavaan::cfa(structure_k6_one, data = test))
#
# # two latents model
# two_latents <-
#   suppressWarnings(lavaan::cfa(structure_k6_two, data = test))
#
# # three latents model
# three_latents <-
#   suppressWarnings(lavaan::cfa(structure_k6_three, data = test))
#
#
# # compare models
# compare <-
#   performance::compare_performance(one_latent, two_latents, three_latents, verbose = FALSE)
#
# # view as html table
# as.data.frame(compare) |>
#   kbl(format = "markdown")
#

# import data and wrangle-------------------------------------------------

library(dplyr)
library(tidyr)



# handle 'ever_had_child'
dat_prep_1 <- dat %>%
  arrange(id, wave) |>
  group_by(id) |>
  mutate(ever_had_child = case_when(
    cummax(replace_na(children_num, -1)) > 0 ~ 1,
    # ever had a child
    cummax(replace_na(children_num, -1)) == 0 &
      !is.na(children_num) ~ 0,
    # reports as 0
    TRUE ~ NA_real_  # NA otherwise
  )) |>
  ungroup()

# check
dat_prep_1 |>
  select(id, wave, year_measured, children_num,  ever_had_child) |>
  print(n = 200)


# update 'child_first' column
# Step: Update 'child_first' using 'ever_had_child'
dat_prep_2 <- dat_prep_1 %>%
  arrange(id, wave) %>%
  group_by(id) %>%
  mutate(child_first = ifelse(lag(ever_had_child) == 0 &
                                ever_had_child == 1, 1, 0)) |>
  mutate(child_new = ifelse(lag(children_num)  <  children_num, 1, 0)) |>
  ungroup()


# check
dat_prep_table_2 <- dat_prep_2 |>
  select(id,
         wave,
         year_measured,
         children_num,
         child_new,
         ever_had_child,
         child_first)

table(dat_prep_table_2$child_first)
table(dat_prep_table_2$child_new)


# new children in 2020

dat_prep_table_2 <- dat_prep_table |>
  filter(wave == 2021, year_measured == 1)

# fairly rare outcome
table(dat_prep_table_2$child_first)


# code for new child within the past two years
# create columns for first child and new child within past 2 waves
dat_prep_3 <- dat_prep_2 |>
  arrange(id, wave) |>
  group_by(id) |>
  arrange(wave) |>
  mutate(
    child_first_past_2_waves = ifelse((lag(child_first, 1) == 1 |
                                         child_first == 1),
                                      1,
                                      0),
    child_new_past_2_waves = (lag(child_new, 1) == 1 |
                                child_new == 1),
    1,
    0
  ) %>%
  ungroup()



dat_prep_table_3 <- dat_prep_3 |>
  filter(wave == 2021)

# note better outcomes
table(dat_prep_table_3$child_first_past_2_waves)

# better outcomes
table(dat_prep_table_3$child_new_past_2_waves)



## DO THIS LATER -- USE A SURVIVAL ANALYSIS

# note create a lead variable to keep data tidy
dat_prep_4 <- dat_prep_3 |>
  arrange(id, wave) |>
  group_by(id) |>
  mutate(
    lead_child_first_past_2_waves = lead(child_first_past_2_waves),
    lead_child_new_past_2_waves = lead(child_new_past_2_waves)
  ) |>
  ungroup()


# check -- we do this to get the 2 year ferility from wave 2020
dat_prep_table_4 <- dat_prep_4 |>
  filter(wave == 2020)

table(dat_prep_table_4$lead_child_first_past_2_waves)
table(dat_prep_table_4$lead_child_new_past_2_waves)



# handle loss to follow-up
dat_long <- dat_prep_4 %>%
  mutate(forgiveness = 8 - vengeful_rumin) |> # reverse score veng rumination
  rowwise(wave) |>
  mutate(power_no_control_composite = mean(c(
    power_self_nocontrol, power_others_control
  ), na.rm = TRUE)) |>
  mutate(kessler_latent_depression =  mean(
    c(kessler_depressed, kessler_hopeless, kessler_worthless),
    na.rm = TRUE
  )) |>
  mutate(kessler_latent_anxiety  = mean(c(
    kessler_effort, kessler_nervous, kessler_restless
  ), na.rm = TRUE)) |>
  ungroup() |>
  mutate(power_no_control_composite_reversed = 8 - power_no_control_composite) |>
  # ungroup
  select(
    "wave",
    "year_measured",
    "id",
    # "edu",
    "sample_origin_names_combined",
    # Sample origin names combined
    #"alert_level_combined_lead",  not needed because all receive all levels by the point the outcome is measured
    # covid alert levels -> 2019-2020
    "education_level_coarsen",
    # Ordinal-Rank 0-10 NZREG codes (with overseas school quals coded as Level 3, and all other ancillary categories coded as missing)  Combined highschool levels See:https://www.nzqa.govt.nz/assets/Studying-in-NZ/New-Zealand-Qualification-Framework/requirements-nzqf.pdf
    "male",
    # 0 = female, 0.5 = neither female nor male, 1 = male.
    "age",
    "born_nz",
    "sexual_orientation",
    "hlth_disability",
    # value label 0    No 1   Yes
    "eth_cat",
    #factor(EthCat, labels = c("Euro", "Maori", "Pacific", "Asian")),
    "employed",
    # Are you currently employed? (this includes self-employment or casual work)
    # "gen_cohort",
    "household_inc",
    # Please estimate your total household income (before tax) for the last year.
    "nz_dep2018",
    # see nzavs materials
    "nzsei13",
    # see nzavs materials
    "partner",
    # 0 = no, 1 = yes
    "parent",
    # 0 = no, 1 = yes
    "political_conservative",
    #Please rate how politically liberal versus conservative you see yourself as being.
    "pol_wing",
    # Please rate how politically left-wing versus right-wing you see yourself as being.
    "urban",
    # see NZAVS,
    "have_siblings",
    #Do you have siblings?
    "total_siblings",
    # sum siblings
    "number_sisters_older",
    #How many older sisters do you have?
    "number_sisters_younger",
    #	How many younger sisters do you have?
    "number_brothers_older",
    #	How many older brothers do you have?
    "number_brothers_younger",
    #	How many older brothers do you have?
    "children_num",
    # How many children have you given birth to, fathered, or adopted?
    "hours_children",
    #Hours - Looking after children
    "hours_work",
    #Hours - Working in paid employment
    "hours_housework",
    # Hours - Housework/cooking
    "hours_exercise",
    "agreeableness",
    # Mini-IPIP6 Agreeableness (also modelled as empathy facet)
    # Sympathize with others' feelings.
    # Am not interested in other people's problems.
    # Feel others' emotions.
    # Am not really interested in others.
    "conscientiousness",
    # see mini ipip6
    # Get chores done right away.
    # Like order.
    # Make a mess of things.
    # Often forget to put things back in their proper place.
    "extraversion",
    # Mini-IPIP6 Extraversion
    # Am the life of the party.
    # Don't talk a lot.
    # Keep in the background.
    # Talk to a lot of different people at parties.
    "honesty_humility",
    # see mini ipip6
    # Would like to be seen driving around in a very expensive car.
    # Would get a lot of pleasure from owning expensive luxury goods.
    # Feel entitled to more of everything.
    # Deserve more things in life.
    "openness",
    # see mini ipip6
    # Have a vivid imagination.
    # Have difficulty understanding abstract ideas.
    # Do not have a good imagination.
    # Am not interested in abstract ideas.
    "neuroticism",
    # see mini ipip6
    # Have frequent mood swings.
    # Am relaxed most of the time.
    # Get upset easily.
    # Seldom feel blue.
    "modesty",
    # see mini ipip6
    # I want people to know that I am an important person of high status,
    # I am an ordinary person who is no better than others.
    # I wouldn’t want people to treat me as though I were superior to them.
    # I think that I am entitled to more respect than the average person is
    # "sdo",
    # "rwa",
    # "brk_relationship",
    # "began_relationship",
    "religion_religious",
    # Do you identify with a religion and/or spiritual group?
    # "religion_religious_not",  # reverse this indicator
    "religion_identification_level",
    #How important is your religion to how you see yourself?"
    "religion_prayer",
    # How many times did you pray in the last week?
    "religion_scripture",
    # How many times did you read religious scripture in the last week?
    "religion_church",
    # How many times did you attend a church or place of worship in the last month?
    "religion_believe_spirit",
    #Do you believe in some form of spirit or lifeforce?
    "religion_believe_spirit",
    #inverse believe in god
    "religion_believe_god",
    #Do you believe in a God
    "religion_believe_god_not",
    #inverse believe in god
    "religion_spiritual_identification",
    #w8,w10,w12-13 "I identify as a spiritual person."
    "religion_perceive_religious_discrim",
    #	I feel that I am often discriminated against because of my religious/spiritual beliefs.
    # "bigger_doms", #What religion or spiritual group?#  Not_Rel, Anglican , Buddist, Catholic , Christian_nfd, Christian_Others, Hindu, Jewish           Muslim, PresbyCongReform, TheOthers
    "w_gend_age_euro",
    # sample_weights.
    # Sometimes I can't sleep because of thinking about past wrongs I have suffered.//# I can usually forgive and forget when someone does me wrong.# I find myself regularly thinking about past times that I have been wronged.
    "gratitude",
    ## I have much in my life to be thankful for. # When I look at the world, I don’t see much to be grateful for. # I am grateful to a wide variety of peopl
    "modesty",
    # see above
    "vengeful_rumin",
    "forgiveness",
    "charity_donate",
    #How much money have you donated to charity in the last year?
    "hours_charity",
    #,#Hours spent in activities/Hours spent … voluntary/charitable work
    "warm_asians",
    "warm_chinese",
    #"warm_disabled" ,  missing at time 0
    # begins w9
    "warm_immigrants",
    "warm_indians",
    "warm_elderly",
    # warm_lgbtq starts w12
    "warm_maori",
    "warm_mental_illness",
    "warm_muslims",
    "warm_nz_euro",
    "warm_overweight",
    "warm_pacific",
    "warm_refugees",
    # "issue_same_sex_marriage", not in range
    "support",
    # three items as below
    # "support_help",
    # # 'There are people I can depend on to help me if I really need it.
    # "support_turnto",
    # # There is no one I can turn to for guidance in times of stress.
    # "support_rnoguidance",
    #There is no one I can turn to for guidance in times of stress.
    "family_time",
    "friends_time",
    "community_time",
    "family_money",
    "friends_money",
    "community_money",
    #Please estimate how much help you have received from the following sources in the last week?
    # Received help and support - hours
    # family
    # friends
    # others in my community
    # Received help and support - money
    # family
    # friends
    # others in my community
    # outcomewide,
    #w8,w10,w12-13 "I identify as a spiritual person."
    "religion_perceive_religious_discrim",
    #	I feel that I am often discriminated against because of my religious/spiritual beliefs.
    # "bigger_doms", #What religion or spiritual group?#  Not_Rel, Anglican , Buddist, Catholic , Christian_nfd, Christian_Others, Hindu, Jewish           Muslim, PresbyCongReform, TheOthers
    # sample_weights
    "alcohol_frequency",
    #"How often do you have a drink containing alcohol?"
    "alcohol_intensity",
    # How many drinks containing alcohol do you have on a typical day when drinking?
    "hlth_bmi",
    # " What is your height? (metres)\nWhat is your weight? (kg)\nKg
    "hours_exercise",
    # Hours spent … exercising/physical activity
    "sfhealth",
    "sfhealth_your_health",
    # "In general, would you say your health is...
    "sfhealth_get_sick_easier_reversed",
    #\nI seem to get sick a little easier than other people.
    "sfhealth_expect_worse_health_reversed",
    #\nI expect my health to get worse." ****
    "hlth_sleep_hours",
    #During the past month, on average, how many hours of actual sleep did you get per night?
    "smoker",
    #Do you currently smoke?
    "hlth_fatigue",
    #During the last 30 days, how often did.... you feel exhausted?
    "rumination",
    "kessler6_sum",
    # depression constructs,
    "kessler_latent_depression",
    "kessler_latent_anxiety",
    # During the last 30 days, how often did.... you have negative thoughts that repeated over and over?
    "kessler_depressed",
    #During the last 30 days, how often did.... you feel so depressed that nothing could cheer you up?
    "kessler_effort",
    #During the last 30 days, how often did.... you feel that everything was an effort?
    "kessler_hopeless",
    # During the last 30 days, how often did.... you feel hopeless?
    "kessler_nervous",
    #During the last 30 days, how often did.... you feel nervous?
    "kessler_restless",
    #During the last 30 days, how often did.... you feel restless or fidgety?
    "kessler_worthless",
    # During the last 30 days, how often did.... you feel worthless?
    "sexual_satisfaction",
    #  How satisfied are you with your sex life?
    "bodysat",
    ## Am satisfied with the appearance, size and shape of my body.
    "vengeful_rumin",
    # Sometimes I can't sleep because of thinking about past wrongs I have suffered.//# I can usually forgive and forget when someone does me wrong.# I find myself regularly thinking about past times that I have been wronged.
    "perfectionism",
    # # Doing my best never seems to be enough./# My performance rarely measures up to my standards.
    # I am hardly ever satisfied with my performance.
    "power_no_control_composite",
    "power_self_nocontrol",
    "power_no_control_composite_reversed",
    # I do not have enough power or control over\nimportant parts of my life.
    "power_others_control",
    # Other people have too much power or control over\nimportant parts of my life
    "self_esteem",
    "selfesteem_satself",
    #  On the whole am satisfied with myself.
    "selfesteem_postiveself",
    # Take a positive attitude toward myself
    "selfesteem_failure_reversed",
    # Am inclined to feel that I am a failure.
    #  "self_control",
    "self_control_have_lots",
    #In general, I have a lot of self-control.
    "self_control_wish_more_reversed",
    #I wish I had more self-discipline.(r)
    "emotion_regulation_out_control",
    # When I feel negative emotions, my emotions feel out of control. w10 - w13
    "emotion_regulation_hide_neg_emotions",
    # When I feel negative emotions, I suppress or hide my emotions. w10 - w13
    "emotion_regulation_change_thinking_to_calm",
    # When I feel negative emotions, I change the way I think to help me stay calm. w10 - w13
    # "emp_work_life_balance",# I have a good balance between work and other important things in my life. # not measured at baseline
    # "respect_self",  #If they knew me, most NZers would respect what I have accomplished in life. Missing at T12
    "gratitude",
    ## I have much in my life to be thankful for. # When I look at the world, I don’t see much to be grateful for. # I am grateful to a wide variety of people.
    #"pwi",
    "pwb_your_health",
    # #Your health.
    "pwb_your_relationships",
    # #Your personal relationships.
    "pwb_your_future_security",
    # #Your future security.
    "pwb_standard_living",
    #Your standard of living.
    "lifesat",
    "lifesat_satlife",
    # I am satisfied with my life.
    "lifesat_ideal",
    # In most ways my life is close to ideal.
    "lifemeaning",
    # average meaning_purpose, meaning_sense
    "meaning_purpose",
    # My life has a clear sense of purpose.
    "meaning_sense",
    # I have a good sense of what makes my life meaningful.
    "permeability_individual",
    #I believe I am capable, as an individual\nof improving my status in society.
    "impermeability_group",
    #The current income gap between New Zealand Europeans and other ethnic groups would be very hard to change.
    "neighbourhood_community",
    #I feel a sense of community with others in my local neighbourhood.
    "support",
    "support_help",
    # 'There are people I can depend on to help me if I really need it.
    "support_turnto",
    # There is no one I can turn to for guidance in times of stress.
    "support_noguidance_reverseed",
    #There is no one I can turn to for guidance in times of stress.
    "belong",
    "belong_accept",
    #Know that people in my life accept and value me.
    "belong_routside_reversed",
    # Feel like an outsider.
    "belong_beliefs",
    # Know that people around me share my attitudes and beliefs.
    "charity_donate",
    #How much money have you donated to charity in the last year?
    "hours_charity",
    #,#Hours spent in activities/Hours spent … voluntary/charitable work
    "nwi",
    # The economic situation in New Zealand./# The social conditions in New Zealand. # Business in New Zealand.
    "emp_job_sat",
    # How satisfied are you with your current job? #Eisenbarth, H., Hart, C. M., Zubielevitch, E., Keilor, T., Wilson, M. S., Bulbulia, J. A., Sibley, C. G., &
    #Sedikides, C. (in press). Aspects of psychopathic personality relate to lower subjective and objective professional success. Personality and Individual Differences, 186, 111340.
    "emp_job_secure",
    #only for employed people
    "emp_job_valued",
    "rural_gch2018",
    "hours_community",
    "hours_friends",
    "hours_family",
    "alert_level_combined_lead",
    "child_first",
    "child_new"
    #"lead_child_first_past_2_waves",  Do later
    #"lead_child_new_past_2_waves"#  Do later
    # Hours spent … socialising with family
    # Hours spent … socialising with friends
    # Hours spent … socialising with community groups
    # Hours spent … socialising with religious groups (only for religion only studies)
  ) |>
  # select variables
  # mutate(across(where(is.double), as.numeric)) |>
  mutate(
    hours_community_log = log(hours_community + 1),
    hours_friends_log = sqrt(hours_friends + 1),
    hours_family_log = sqrt(hours_family + 1)
  ) |>
  mutate(male = as.numeric(male)) |>
  mutate(total_siblings_factor = ordered(round(
    ifelse(total_siblings > 7, 7, total_siblings), 0
  ))) |>
  mutate(religion_prayer_binary = ifelse(religion_prayer > 0, 1, 0)) |>
  mutate(religion_church_binary = ifelse(religion_church > 0, 1, 0))  |>
  mutate(
    religion_church_f = ifelse(religion_church >= 4, 4, religion_church),
    religion_church_f = round(religion_church_f, 0)
  ) |>
  mutate(religion_church_f = factor(religion_church_f, ordered = TRUE)) |>
  mutate(religion_scripture_binary = ifelse(religion_scripture > 0, 1, 0)) |>
  mutate(religion_church_round = round(ifelse(religion_church >= 8, 8, religion_church), 0)) |>
  mutate(hours_community_round = round(ifelse(hours_community >= 24, 24, hours_community), 0)) |>
  mutate(
    eth_cat = as.integer(eth_cat),
    urban = as.numeric(urban),
    education_level_coarsen = as.integer(education_level_coarsen)
  ) |>
  dplyr::filter((wave == 2018 & year_measured  == 1) |
                  (wave == 2019  &
                     year_measured  == 1) |
                  (wave == 2020)) |>  # Eligibility criteria  Observed in 2018/2019 & Outcomes in 2020 or 2021
  group_by(id) |>
  ## MAKE SURE YOU HAVE ELIGIBILITY CRITERIA
  dplyr::mutate(meets_criteria_baseline = ifelse(year_measured == 1 &
                                                   !is.na(!!sym(nzavs_exposure)), 1, 0)) |>  # using R lang
  filter((wave == 2018 & year_measured == 1) |
           (wave == 2019 & year_measured == 1) |
           (wave == 2020)) %>%
  group_by(id) |>
  mutate(k_18 = ifelse(wave == 2018 &
                         meets_criteria_baseline == 1, 1, 0)) %>% # selection criteria
  mutate(h_18 = mean(k_18, na.rm = TRUE)) %>%
  mutate(k_19 = ifelse(wave == 2019 &
                         meets_criteria_baseline == 1, 1, 0)) %>% # selection criteria
  mutate(h_19 = mean(k_19, na.rm = TRUE)) %>%
  dplyr::filter(h_18 > 0) |>  # hack to enable repeat of baseline
  dplyr::filter(h_19 > 0) |>  # hack to enable repeat of baseline
  ungroup() %>%
  mutate(
    not_lost = ifelse(lead(year_measured) == 1, 1, 0),
    # not_lost = ifelse(lead(year_measured)== -1, 0, not_lost,
    # not_lost = ifelse(lead(year_measured) == 0, 0, not_lost,
    not_lost = ifelse(is.na(not_lost) &
                        year_measured == 1, 1, not_lost),
    not_lost = ifelse(is.na(not_lost), 0, not_lost)
  ) |>
  ungroup() |>
  dplyr::mutate(
    friends_money = ifelse(friends_money < 0, 0, friends_money),
    # someone gave neg number
    household_inc_log = log(household_inc + 1),
    hours_children_log = log(hours_children + 1),
    hours_work_log = log(hours_work + 1),
    hours_housework_log = log(hours_housework + 1),
    hours_exercise_log = log(hours_exercise + 1)
  ) |>
  dplyr::mutate(religion_church_four = as.numeric(religion_church_f) - 1) |>
  dplyr::rename(sample_weights = w_gend_age_euro) |>
  dplyr::mutate(sample_origin = as.factor(sample_origin_names_combined)) |>  #shorter name
  arrange(id, wave) |>
  droplevels() |>
  select(-h_18, -k_18, -h_19, -k_19) |>
  droplevels() |>
  ungroup() %>%
  mutate(time = as.numeric(wave) - 1) |>
  arrange(id, wave) |>
  #   mutate(
  #   religion_church_coarsen = cut(
  #     religion_church,
  #     breaks = c(-Inf, 0, 1, 3.99, Inf),
  #     labels = c("zero", "one", "less_four", "four_up"),
  #     include.lowest = TRUE,
  #     right = TRUE,
  #     ordered = TRUE
  #   )
  # ) %>%
  # mutate(
#   religion_church_coarsen_n = as.numeric(religion_church_coarsen) - 1,
#   religion_church_binary_n = as.numeric(religion_church_binary)
# ) |>
mutate(
  # eth_cat = as.integer(eth_cat),
  urban = as.numeric(urban),
  education_level_coarsen = as.integer(education_level_coarsen)
) |>
  droplevels() |>
  arrange(id, wave) |>
  data.frame()

table(dat_long$time)
table(dat_long$wave)


N <- n_unique(dat_long$id)
N


# factors
#
# dt_only_k6 <- dt_19 |> select(kessler_depressed, kessler_effort,kessler_hopeless,
#                                  kessler_worthless, kessler_nervous,
#                                  kessler_restless)
#
#
# # check factor structure
# performance::check_factorstructure(dt_only_k6)
#
# # explore a factor structure made of 3 latent variables
# efa <- psych::fa(dt_only_k6, nfactors = 2) %>%
#   model_parameters(sort = TRUE, threshold = "max")
#
# efa
#
#
# n <- n_factors(dt_only_k6)
#
# # plot
# plot(n) + theme_classic()
#
# # CFA
# part_data <- datawizard::data_partition(dt_only_k6, traing_proportion = .7, seed = seed)
#
#
# # set up training data
# training <- part_data$p_0.7
# test <- part_data$test
#
#
# # one factor model
# structure_k6_one <- psych::fa(training, nfactors = 1) |>
#   efa_to_cfa()
#
# # two factor model model
# structure_k6_two <- psych::fa(training, nfactors = 2) |>
#   efa_to_cfa()
#
# # three factor model
# structure_k6_three <- psych::fa(training, nfactors = 3) %>%
#   efa_to_cfa()
#
# # inspect models
# structure_k6_one
# structure_k6_two
# structure_k6_three
#
#
# # Next we perform the confirmatory factor analysis.
#
#
# one_latent <-
#   suppressWarnings(lavaan::cfa(structure_k6_one, data = test))
#
# # two latents model
# two_latents <-
#   suppressWarnings(lavaan::cfa(structure_k6_two, data = test))
#
# # three latents model
# three_latents <-
#   suppressWarnings(lavaan::cfa(structure_k6_three, data = test))
#
#
# # compare models
# compare <-
#   performance::compare_performance(one_latent, two_latents, three_latents, verbose = FALSE)
#
# # view as html table
# as.data.frame(compare) |>
#   kbl(format = "markdown")
#

nzavs_exposure
table(dat_long$religion_church_four)

dt_19 <- dat_long |>
  filter(year_measured == 1 & wave == 2019) |>
  mutate(Church_Attendance_Grouped = religion_church_four)

hist(dt_19$Church_Attendance_Grouped)


# generate bar plot
graph_density_of_exposure <-
  coloured_histogram(dt_19,
                     col_name = "Church_Attendance_Grouped",
                     scale_min = 0,
                     scale_max  = 4)

graph_density_of_exposure

push_mods

ggsave(
  graph_density_of_exposure,
  path = here::here(here::here(push_mods, "figs")),
  width = 12,
  height = 8,
  units = "in",
  filename = "graph_density_of_exposure.png",
  device = 'png',
  limitsize = FALSE,
  dpi = 600
)



nzavs_exposure


# double check path
push_mods

dev.off()

# check

dt_check_exposure <-
  dat_long |>   filter(wave == 2018 | wave == 2019)

# makes sure all is false
table (is.na(dt_check_exposure$Church_Attendance_Grouped))

dt_positivity_full <- dat_long |>
  filter(wave == 2018 | wave == 2019) |>
  select(wave,
         id,
         religion_church_four,
         religion_church_round)
dt_positivity_full

# no need for the rounded variable
table(dt_positivity_full$religion_church_round)

# check sample weights NA - will return to this after impute
table (is.na(dt_positivity_full$sample_weights)) #

# test positivity
out <-
  msm::statetable.msm(religion_church_round, id, data = dt_positivity_full)

# transition table
t_tab <- transition_table(out, state_names = NULL)
t_tab



# test positivity
out <-
  msm::statetable.msm(religion_church_four, id, data = dt_positivity_full)

# transition table
t_tab <- transition_table(out, state_names = NULL)
t_tab


# check outcome
dt_check_outcome <-
  dat_long |>   filter(wave == 2020)



table(dt_check_outcome$child_first)
table(dt_check_outcome$child_new)

# set variables for baseline exposure and outcome -------------------------

baseline_vars = c(
  "male",
  "age",
  "education_level_coarsen",
  # factors
  "eth_cat",
  #factor(EthCat, labels = c("Euro", "Maori", "Pacific", "Asian")),
  #"bigger_doms", #religious denomination
  "sample_origin",
  "nz_dep2018",
  "nzsei13",
  "born_nz",
  "hlth_disability",
  # "hlth_bmi",
  # "pwi", # pwi
  # "kessler6_sum",
  "kessler_latent_depression",
  "kessler_latent_anxiety",
  "support",
  #soc support
  "belong",
  # social belonging
  "total_siblings_factor",
  #  "smoker", # smoker
  # "sfhealth",
  # "alcohol_frequency", measured with error
  # "alcohol_intensity",
  # "hours_family_log",
  # "hours_friends_log",
  # "hours_community_log",
  # "hours_community_sqrt_round",
  # "lifemeaning",
  "household_inc_log",
  # added: measured with error but OK for imputations
  "partner",
  # "parent",  # newly changed - have information in child number
  "political_conservative",
  #Please rate how politically liberal versus conservative you see yourself as being.
  # Sample origin names combined
  "urban",
  "children_num",
  "hours_children_log",
  # new
  "hours_work_log",
  # new
  "hours_housework_log",
  #new
  "hours_exercise_log",
  "agreeableness",
  "conscientiousness",
  "extraversion",
  "honesty_humility",
  "openness",
  "neuroticism",
  "modesty",
  # I want people to know that I am an important person of high status, I am an ordinary person who is no better than others. , I wouldn’t want people to treat me as though I were superior to them. I think that I am entitled to more respect than the average person is.
  # "religion_religious", # Do you identify with a religion and/or spiritual group?
  # "religion_identification_level", #How important is your religion to how you see yourself?"  # note this is not a great measure of virtue, virtue is a mean between extremes.
  "religion_church_round",
  # "religion_religious", #
  # "religion_spiritual_identification",
  # "religion_identification_level",
  #  "religion_religious",
  #  "religion_church_binary",
  #  "religion_prayer_binary",
  #  "religion_scripture_binary",
  #"religion_believe_god",
  #"religion_believe_spirit",
  "sample_weights",
  "alert_level_combined_lead"
)


# check
baseline_vars


# check
exposure_var

# outcomes
outcome_vars = c("child_first",
                 "child_new")
# impute baseline data (we use censoring for the outcomes)
#colnames(dat_long)
# function imputes only baseline not outcome

# rename(new_child_past_2_waves_2021 =  "lead_new_child_past_2_waves",
#        first_child_past_2_waves_2021 = "lead_first_child_past_2_waves") |>

# make data wide and impute baseline missing values -----------------------
baseline_vars

exposure_var

# custom function
prep_coop_all <- margot_wide_impute_baseline(
  dat_long,
  baseline_vars = baseline_vars,
  exposure_var = exposure_var,
  outcome_vars = outcome_vars
)


table(is.na(prep_coop_all$t2_lead_new_child_past_2_waves))
table(is.na(prep_coop_all$t2_lead_first_child_past_2_waves))

# remove baseline lead variable



# check mi model
# outlist <-
#   row.names(prep_coop_all)[prep_coop_all$outflux < 0.5]
# length(outlist)
#
# # checks. We do not impute with weights: area of current research
# head(prep_coop_all$loggedEvents, 10)

push_mods

naniar::vis_miss(prep_coop_all, warn_large_data = FALSE)
dev.off()

# make edu an ordered factor
unique_levels <-
  sort(unique(prep_coop_all$t0_education_level_coarsen))

prep_coop_all$t0_education_level_coarsen <-
  factor(prep_coop_all$t0_education_level_coarsen,
         levels = unique_levels,
         ordered = TRUE)

# save function -- will save to your "push_mod" directory



here_save(prep_coop_all, "prep_coop_all")

# read function
prep_coop_all <- here_read("prep_coop_all")

head(prep_coop_all)



#check must be a dataframe
str(prep_coop_all)
nrow(prep_coop_all)
colnames(prep_coop_all)
prep_coop_all <- as.data.frame(prep_coop_all)

table()

# arrange data for analysis -----------------------------------------------
# spit and shine

str(prep_coop_all$t0_education_level_coarsen)

df_wide_censored <-
  prep_coop_all |>
  mutate(t0_eth_cat = as.factor(t0_eth_cat)) |>
  relocate("t0_not_lost", .before = starts_with("t1_"))  %>%
  relocate("t1_not_lost", .before = starts_with("t2_"))

#check
head(df_wide_censored)
dim(df_wide_censored)
str(df_wide_censored)

colnames(df_wide_censored)
# spit and shine
df_clean <- df_wide_censored %>%
  mutate(t2_na_flag = rowSums(is.na(select(
    ., starts_with("t2_")
  ))) > 0) %>%
  mutate(t1_not_lost = ifelse(t2_na_flag, 0, t1_not_lost)) %>%
  # select(-t2_na_flag) %>%
  filter(!rowSums(is.na(select(
    ., starts_with("t0_")
  )))) |>
  dplyr::mutate(
    across(
      where(is.numeric) &
        !t0_religion_church_four &
        !t0_religion_church_round &
        !t0_religion_church &
        !t0_religion_church_four &
        !t0_sample_weights &
        !t1_religion_church_four &
        !t0_not_lost &
        !t1_not_lost &
        !t2_child_first &
        !t2_child_new,
      ~ scale(.x),
      .names = "{col}_z"
    )
  ) |>
  select(
    where(is.factor),
    #    t0_smoker_binary,
    t0_not_lost,
    t0_sample_weights,
    #  t1_permeability_individual, # make sure to change for each study
    # t0_religion_identification_level,
    t0_religion_church_four,
    t0_religion_church_round,
    t0_religion_church,
    t1_religion_church_four,
    t1_religion_church_round,
    t1_religion_church,
    t1_not_lost,
    t2_child_new,
    t2_child_first,
    ends_with("_z")
  ) |>
  relocate(starts_with("t0_"), .before = starts_with("t1_"))  %>%
  relocate(starts_with("t2_"), .after = starts_with("t1_"))  %>%
  relocate("t0_not_lost", .before = starts_with("t1_"))  %>%
  relocate("t1_not_lost", .before = starts_with("t2_")) |>
  mutate(t0_sample_weights = as.numeric(t0_sample_weights)) |>
  data.frame()

dim(df_clean)
naniar::vis_miss(df_clean, warn_large_data = FALSE)
dev.off()


# again check path
push_mods
# save
here_save(df_clean, "df_clean")


str(df_clean)
df_clean <- as.data.frame(df_clean)

#check n
N <- nrow(df_clean)
N

colnames(df_clean)
# get names
names_base <-
  df_clean |> select(
    starts_with("t0"),
    -t0_child_new_z,
    -t0_child_first_z,
    -t0_religion_church_four,
    -t0_religion_church_round,
    -t0_not_lost
  ) |> colnames()

names_base

names_outcomes <-
  df_clean |> select(starts_with("t2")) |> colnames()

names_outcomes

colnames(df_clean)

# set variables for models ------------------------------------------------

#### SET VARIABLE NAMES: Customise for each outcomewide model
#  model
df_clean
A <- "t1_religion_church_four"
A_1 <- "t1_religion_church_round"
A_2 <- "t1_religion_church_round"




C <- c("t1_not_lost")

#L <- list(c("L1"), c("L2"))
W <- c(paste(names_base, collapse = ", "))

# check
print(W)


f
f_1
# make test data (if needed)
df_clean_test <- df_clean |>
  slice_head(n = 2000)


# create baselines
names_base_A <-
  df_clean |> select(
    starts_with("t0"),
    -t0_child_new_z,
    -t0_child_first_z,
    -t0_religion_church,
    -t0_religion_church_round,
    -t0_not_lost
  ) |> colnames()

names_base_A

names_base_A_1 <-
  df_clean |> select(
    starts_with("t0"),
    -t0_child_new_z,
    -t0_child_first_z,
    -t0_religion_church_four,
    -t0_religion_church,
    -t0_not_lost
  ) |> colnames()
names_base_A_1

names_base_A_2 <-
  df_clean |> select(
    starts_with("t0"),
    -t0_child_new_z,
    -t0_child_first_z,
    -t0_religion_church_four,
    -t0_religion_church_round,
    -t0_not_lost
  ) |> colnames()
names_base_A_2


## Tests using less data

#
#
# test_timing_info_f_A <- system.time({
#   test_t2_child_first_f_A <- lmtp_tmle(
#     data = df_clean_test,
#     trt = A,
#     baseline = names_base_A,
#     outcome = "t2_child_first",
#     cens = C,
#     shift = f,
#     mtp = TRUE,
#     folds = 5,
#     # trim = 0.99, # if needed
#     # time_vary = NULL,
#     outcome_type = "binomial",
#     #  id = "id",
#     weights = df_clean_test$t0_sample_weights,
#     learners_trt = sl_lib,
#     learners_outcome = sl_lib,
#     parallel = n_cores
#   )
# })
#
#
# test_t2_child_first_f_A
# here_save(test_t2_child_first_f_A, "test_t2_child_first_f_A")
# test_t2_child_first_f_A <- here_read("test_t2_child_first_f_A")
#
#
#
# # user  system elapsed
# # 5.466   0.621 275.792
#
# test_timing_info_f_A_1 <- system.time({
#   test_t2_child_first_f_A_1 <- lmtp_tmle(
#     data = df_clean_test,
#     trt = A_1,
#     baseline = names_base_A_1,
#     outcome = "t2_child_first",
#     cens = C,
#     shift = f,
#     mtp = TRUE,
#     folds = 5,
#     # trim = 0.99, # if needed
#     # time_vary = NULL,
#     outcome_type = "binomial",
#     #  id = "id",
#     weights = df_clean_test$t0_sample_weights,
#     learners_trt = sl_lib,
#     learners_outcome = sl_lib,
#     parallel = n_cores
#   )
# })
#
#
# test_t2_child_first_f_A_1
# here_save(test_t2_child_first_f_A, "test_t2_child_first_f_A_1")
# test_t2_child_first_f_A_1 <- here_read("test_t2_child_first_f_A_1")
#
#
# print(paste("Time taken: ", round(test_timing_info_f_A_1['elapsed'], 2), " seconds"))
#
#
#
#
# test_timing_info_f_A_2 <- system.time({
#   test_t2_child_first_f_A_2 <- lmtp_tmle(
#     data = df_clean_test,
#     trt = A_2,
#     baseline = names_base_A,
#     outcome = "t2_child_first",
#     cens = C,
#     shift = f,
#     mtp = TRUE,
#     folds = 5,
#     # trim = 0.99, # if needed
#     # time_vary = NULL,
#     outcome_type = "binomial",
#     #  id = "id",
#     weights = df_clean_test$t0_sample_weights,
#     learners_trt = sl_lib,
#     learners_outcome = sl_lib,
#     parallel = n_cores
#   )
# })
#
# warnings()
# test_t2_child_first_f_A_2
# here_save(test_t2_child_first_f_A_2, "test_t2_child_first_f_A_2")
#
# print(paste("Time taken: ", round(test_timing_info_f_A['elapsed'], 2), " seconds"))
#
#
#



# print(paste("Time taken: ", round(test_timing_info_f_A_2['elapsed'], 2), " seconds"))
# [1] "Time taken:  351.28  seconds"
# > print(paste("Time taken: ", round(test_timing_info_f_A_1['elapsed'], 2), " seconds"))
# [1] "Time taken:  332.07  seconds"
# > print(paste("Time taken: ", round(test_timing_info_f_A['elapsed'], 2), " seconds"))
# [1] "Time taken:  275.79  seconds"

#
# test_diff <-
#   lmtp_contrast(test_t2_child_first_f_A_1,
#                 ref = test_t2_child_first_f_A_2,
#                 type = "rr")
#
# test_diff
#




# second test intervention -------------------------------------------------


# health models -----------------------------------------------------------



# first test intervention -------------------------------------------------

## USE A_1
names_base_A_1
A_1

t2_child_first_f_A_1 <- lmtp_tmle(
  data = df_clean,
  trt = A_1,
  baseline = names_base_A_1,
  outcome = "t2_child_first",
  cens = C,
  shift = f,
  mtp = TRUE,
  folds = 5,
  # trim = 0.99, # if needed
  # time_vary = NULL,
  outcome_type = "binomial",
  #  id = "id",
  weights = df_clean$t0_sample_weights,
  learners_trt = sl_lib,
  learners_outcome = sl_lib,
  parallel = n_cores
)



# check model
t2_child_first_f_A_1

# save
here_save(t2_child_first_f_A_1, "t2_child_first_f_A_1")




t2_child_first_f_1_A_1 <- lmtp_tmle(
  data = df_clean,
  trt = A_1,
  baseline = names_base_A_1,
  outcome = "t2_child_first",
  cens = C,
  shift = f_1,
  mtp = TRUE,
  folds = 5,
  # trim = 0.99, # if needed
  # time_vary = NULL,
  outcome_type = "binomial",
  #  id = "id",
  weights = df_clean$t0_sample_weights,
  learners_trt = sl_lib,
  learners_outcome = sl_lib,
  parallel = n_cores
)



# check model
t2_child_first_f_1_A

# save
here_save(t2_child_first_f_1_A, "t2_child_first_f_1_A")




# null
t2_child_first_null_A_1 <- lmtp_tmle(
  data = df_clean,
  trt = A_1,
  baseline = names_base_A_1,
  outcome = "t2_child_first",
  cens = C,
  shift = NULL,
  mtp = FALSE,
  folds = 5,
  # trim = 0.99, # if needed
  # time_vary = NULL,
  outcome_type = "binomial",
  #  id = "id",
  weights = df_clean$t0_sample_weights,
  learners_trt = sl_lib,
  learners_outcome = sl_lib,
  parallel = n_cores
)



# check model
t2_child_first_null_A_1

# save
here_save(t2_child_first_null_A_1, "t2_child_first_null_A_1")




# new child ---------------------------------------------------------------



t2_child_new_f_A_1 <- lmtp_tmle(
  data = df_clean,
  trt = A_1,
  baseline = names_base_A_1,
  outcome = "t2_child_new",
  cens = C,
  shift = f,
  mtp = TRUE,
  folds = 5,
  # trim = 0.99, # if needed
  # time_vary = NULL,
  outcome_type = "binomial",
  #  id = "id",
  weights = df_clean$t0_sample_weights,
  learners_trt = sl_lib,
  learners_outcome = sl_lib,
  parallel = n_cores
)




# check model
t2_child_new_f_A_1

# save
here_save(t2_child_new_f_A_1, "t2_child_new_f_A_1")




t2_child_new_f_1_A_1 <- lmtp_tmle(
  data = df_clean,
  trt = A_1,
  baseline = names_base_A_1,
  outcome = "t2_child_new",
  cens = C,
  shift = f_1,
  mtp = TRUE,
  folds = 5,
  # trim = 0.99, # if needed
  # time_vary = NULL,
  outcome_type = "binomial",
  #  id = "id",
  weights = df_clean$t0_sample_weights,
  learners_trt = sl_lib,
  learners_outcome = sl_lib,
  parallel = n_cores
)



# check model
t2_child_new_f_1_A_1

# save
here_save(t2_child_new_f_1_A_1, "t2_child_new_f_1_A_1")



t2_child_new_null_A_1 <- lmtp_tmle(
  data = df_clean,
  trt = A_1,
  baseline = names_base_A_1,
  outcome = "t2_child_new",
  cens = C,
  shift = NULL,
  mtp = FALSE,
  folds = 5,
  # trim = 0.99, # if needed
  # time_vary = NULL,
  outcome_type = "binomial",
  #  id = "id",
  weights = df_clean$t0_sample_weights,
  learners_trt = sl_lib,
  learners_outcome = sl_lib,
  parallel = n_cores
)


# check model
t2_child_new_null_A_1

# save
here_save(t2_child_new_null_A_1, "t2_child_new_null_A")



# Contrast: child first -----------------------------------------------------


t2_child_first_f_A_1 <- here_read("t2_child_first_f_A_1")
t2_child_new_f_1_A_1 <- here_read("t2_child_new_f_1_A_1")
t2_child_new_null_A_1 <-
  here_read("t2_child_new_null_A_1")



# contrast both
contrast_t2_child_new_both <-
  lmtp_contrast(t2_child_first_f_A_1,
                ref = t2_child_new_null_A_1,
                type = "rr")


tab_contrast_t2_child_new_both <-
  margot_tab_lmtp(contrast_t2_child_new_both,
                  scale = "RR",
                  new_name = "New Child")

tab_contrast_t2_child_new_both


out_tab_contrast_t2_child_new_both <-
  lmtp_evalue_tab(tab_contrast_t2_child_new_both,
                  scale = c("RR"))

out_tab_contrast_t2_child_new_both


# contrast down null
contrast_t2_child_new_down_null <-
  lmtp_contrast(t2_child_new_f_A_1,
                ref = t2_child_new_null_A_1,
                type = "rr")


tab_contrast_t2_child_new_down_null <-
  margot_tab_lmtp(contrast_t2_child_new_down_null,
                  scale = "RR",
                  new_name = "New Child")

tab_contrast_t2_child_new_down_null


out_tab_contrast_t2_child_new_down_null <-
  lmtp_evalue_tab(tab_contrast_t2_child_new_down_null,
                  scale = c("RR"))

out_tab_contrast_t2_child_new_down_null




# Contrast: child any -----------------------------------------------------




t2_child_new_f_A_1 <- here_read("t2_child_new_f_A_1")
t2_child_new_f_1_A_1 <- here_read("t2_child_new_f_1_A_1")
t2_child_new_null_A_1 <-
  here_read("t2_child_new_null_A_1")



# contrast both
# contrast down null
contrast_t2_child_new_both <-
  lmtp_contrast(t2_child_new_f_A_1,
                ref = t2_child_new_f_1_A_1,
                type = "rr")


tab_contrast_t2_child_new_both <-
  margot_tab_lmtp(contrast_t2_child_new_both,
                  scale = "RR",
                  new_name = "New Child")

tab_contrast_t2_child_new_both


out_tab_contrast_t2_child_new_both <-
  lmtp_evalue_tab(tab_contrast_t2_child_new_both,
                  scale = c("RR"))

out_tab_contrast_t2_child_new_both


# contrast down null
contrast_t2_child_new_down_null <-
  lmtp_contrast(t2_child_new_f_A_1,
                ref = t2_child_new_null_A_1,
                type = "rr")


tab_contrast_t2_child_new_down_null <-
  margot_tab_lmtp(contrast_t2_child_new_down_null,
                  scale = "RR",
                  new_name = "New Child")

tab_contrast_t2_child_new_down_null


out_tab_contrast_t2_child_new_down_null <-
  lmtp_evalue_tab(tab_contrast_t2_child_new_down_null,
                  scale = c("RR"))

out_tab_contrast_t2_child_new_down_null




# contrast up null
contrast_t2_child_new_up_null <-
  lmtp_contrast(t2_child_new_f_1_A_1,
                ref = t2_child_new_null_A_1,
                type = "rr")


tab_contrast_t2_child_new_up_null <-
  margot_tab_lmtp(contrast_t2_child_new_up_null,
                  scale = "RR",
                  new_name = "New Child")

tab_contrast_t2_child_new_up_null


out_tab_contrast_t2_child_new_up_null <-
  lmtp_evalue_tab(tab_contrast_t2_child_new_up_null,
                  scale = c("RR"))

out_tab_contrast_t2_child_new_up_null
